# Multiple and Polinomial Regression

In An Introduction to Statistical Learning External link 2nd Edition, read the following sections:

3.2 Multiple Linear Regression (pages 71-72)
3.2.1 Estimating the Regression Coefficients (pages 72-75)
7.1 Polynomial Regression (pages 290-292)

## Notes
![Formula](images\formula.png)

## Some Important Questions
When we perform multiple linear regression, we usually are interested in
answering a few important questions.
1. Is at least one of the predictors X1, X2, . . . , Xp useful in predicting
the response?
2. Do all the predictors help to explain Y , or is only a subset of the
predictors useful?
3. How well does the model fit the data?
4. Given a set of predictor values, what response value should we predict,
and how accurate is our prediction?


# 2.1 Multilinear Regression

We will always want as many data as possible to build a model. 
If we get to the point that a feature seams to be insignificant, we just ignores it.

Vector notation is used for multilenar regression



# 2.2 Techniques for Multilinear Modeling

qualitative variables must be converted to dummy variables (one hot encoder)


## Type 1 and Type 2 Errors
Type of error matters

![Confusion matrix for desease model](images/confusion%20matrix%20for%20desease%20model.png)

More about the concept of Confusion Matrix in [Fundamentos de aprendizado de máquina: a matriz de confusão](https://www.youtube.com/watch?v=Kdsp6soqA7o)

How to plot a confusion matrix in Python? [How to Plot Confusion Matrix Heatmap in Python](https://www.youtube.com/watch?v=98LX2iRWXsY)


## Scaling: Normalization and Standardization

not needed for a true linear problem, but often required in a polynomial problem

Min-Max Scaller

Standard Scaler

![Standarize and Normalize](images/standardize%20and%20normalize%20.png)

Explanation on: [Standardization vs Normalization Clearly Explained!](https://www.youtube.com/watch?v=sxEqtjLC0aM)

## Multicolinearity
Multicolinearity makes hard the job to determine what input had more influence in outcome

Generaly, we would like to keep just one of the colinear variables

use pairplots to check that 

more on Multicolinearity: [Multicollinearity (in Regression Analysis)](https://www.youtube.com/watch?v=G1WX5GiFSWQ)

VIF is a measure of multicolinearity. More than 10 indicates that.

doesn't affect prediction, but must be handled if determinination of what features is important is needed

# 2.3 Polynomial Regression

It is often a good idea to scale the input of a polynomial regression. To large or too small numbers to the 20th power will underflow or overflows the math.

how to implment Polynomial Regression using SkLearn: [Machine Learning with Scikit-Learn Python | Polynomial Linear Regression](https://www.youtube.com/watch?v=wi6VoJcLyag)


## Underfitting and Overfitting




# Practices

## Features: Must We Pick Just One?

## Fitting a Multi-Regression Model

## Creating Dummy/Indicator Variables

## Features on Different Scales

## Multi-collinearity vs Model Predictions

## A Line Won't Cut It

## Polynomial Modeling
sometimes we can get one more coeficients than expected. This can happen because sklearn transformations include a column with 1s for the y intercept.
Solve that with: include_bias=False




# Kaggle
Projects that I looked over to see these topics in practice

Multilenear projects
* [Fish Market-Multilinear Regression](https://www.kaggle.com/code/hakansaritas/fish-market-multilinear-regression)
  * > Skewed analisys, Multicolinearity. Models with SMF, Sckitlearn. Visualization of predicted Weight
* [The power of normalization and visualization](https://www.kaggle.com/code/fightingmuscle/the-power-of-normalization-and-visualization?scriptVersionId=92329738)
  * > 
* [Bike Sharing : Multiple Linear Regression](https://www.kaggle.com/code/gauravduttakiit/bike-sharing-multiple-linear-regression)
  * > 

Scaling and normalization
* [Scaling and Normalization](https://www.kaggle.com/code/alexisbcook/scaling-and-normalization)
  * > 
* [Data Cleaning Challenge: Scale and Normalize Data](https://www.kaggle.com/code/rtatman/data-cleaning-challenge-scale-and-normalize-data)
  * > 
* [Complete guide to Feature Scaling](https://www.kaggle.com/code/aimack/complete-guide-to-feature-scaling?scriptVersionId=70328903)
  * > 


* [Titanic Data Science Solutions](https://www.kaggle.com/code/startupsci/titanic-data-science-solutions)
  * > 

Encoders
* [11 Categorical Encoders and Benchmark](https://www.kaggle.com/code/subinium/11-categorical-encoders-and-benchmark)
  * > 

Confusion Matrix
* [Bank Marketing + Classification + ROC,F1,RECALL...](https://www.kaggle.com/code/henriqueyamahata/bank-marketing-classification-roc-f1-recall)
  * > 

Multicolinearity
* [Multicolinearity Problem | SVM (RBF)](https://www.kaggle.com/code/heitornunes/multicolinearity-problem-svm-rbf)
  * > 

Polinomial
* [Polynomial Regression](https://www.kaggle.com/code/aminizahra/polynomial-regression)
  * > 
* [Polynomial Regression| Regularization| Assumptions](https://www.kaggle.com/code/farzadnekouei/polynomial-regression-regularization-assumptions)
  * > 


Extra:
* [Forecasting of Bitcoin Prices](https://www.kaggle.com/code/ara0303/forecasting-of-bitcoin-prices)
  * > 

## My KNN Project
 

## My Linear Regression Project