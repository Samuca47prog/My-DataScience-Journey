# Model Selection and Cross-Validation

In An Introduction to Statistical Learning External link, 2nd edition, read the following sections:

Section 5.1.1 Validation Set Approach (page 198)

# 3.1 Model Selection

Just lower loss function doens't mean its the best model. 
We evaluate fitting in graph and cross validation

whe want the model that best generalize the data

Least underfitted, not overfitted
![Train, validation, Test]("../images/best-model.png")

never use the test set to train nor select the model

# 3.2 Cross-Validation

Use train_test_split to separate train, validation and test 
![Train, validation, Test]("../images/train_test_validation.png")


When to use cross validation?
its always better if we user cross-validation, but in cases that training takes too long, we may not use it.

# Exercises
## Model Selection
## Model Selection using validation
## Model Selection using Cross-validation


# Kaggle
Model Selection
* [Quora EDA & Model selection (ROC, PR plots)](https://www.kaggle.com/code/philschmidt/quora-eda-model-selection-roc-pr-plots)
    * >

* [Feature Selection and Data Visualization](https://www.kaggle.com/code/kanncaa1/feature-selection-and-data-visualization)
    * >

Cross-Validation
* [Cross-Validation](https://www.kaggle.com/code/dansbecker/cross-validation)
    * >

Hyperparameters
* [ObesityDataSet: EDA, Data Prep, ML & HyperTuning](https://www.kaggle.com/code/pmrich/obesitydataset-eda-data-prep-ml-hypertuning)
    * > 

extra
* [A Data Science Framework: To Achieve 99% Accuracy](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)
    * >