# Bias, Variance, and Hyperparameters
In An Introduction to Statistical Learning External link 2nd Edition, read the following sections:

2.2.2 Bias Variance Tradeoff (pages 33-36)
Optional: 6.2.1-6.2.2: Ridge and Lasso Regression - Estimating the Regression Coefficients (pages 215-227)
Optional: [Regularization in Machine Learning](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)


It is a good practice to use the functions built in Libraries, because they are optimized

# 4.1 Introductory Exercise



# 4.2 Bias and Variance

Bias is the error on training data of the model
The more complex the model, the lower will be the Bias


Variance is the range of different models we build on a dataset.
The more comples our model, the more variance we will have 

# 4.3 Ridge and LASSO 

![Ridge vs Lasso](..\images/Rifge%20vs%20lasso.pngmages/)


# Exercises

## Variation of coefficients
## Bias Variance Tradeoff
## Regularization with Cross-validation
## Hyper-parameter Tuning for Ridge Regression
## Simple Lasso and Ridge Regularization




# Kaggle

Bias-Variance
* [ML Basics - BIAS & VARIANCE TRADEOFF](https://www.kaggle.com/code/kaanboke/ml-basics-bias-variance-tradeoff)
  * > 
  
* [Mastering Bias-Variance Tradeoff](https://www.kaggle.com/code/azminetoushikwasi/mastering-bias-variance-tradeoff)
  * > 
  

Hyperparameter tuning
* [A Guide on XGBoost hyperparameters tuning](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning)
  * >

Ridge and Lasso
* [Basic Linear, Ridge and Lasso Regression](https://www.kaggle.com/code/ishaanthareja007/basic-linear-ridge-and-lasso-regression)
  * > Simplest implementation

* [Linear Regression & Regularization(Lasso & Ridge)](https://www.kaggle.com/code/niteshyadav3103/linear-regression-regularization-lasso-ridge)
  * > 

* [Regularized Linear Models](https://www.kaggle.com/code/apapiu/regularized-linear-models)
  * > 