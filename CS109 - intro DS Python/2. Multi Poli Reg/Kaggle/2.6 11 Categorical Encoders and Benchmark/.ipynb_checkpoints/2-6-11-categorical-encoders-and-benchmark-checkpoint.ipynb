{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Inspired by this article and the repo, I have created the following kernel:\n",
    "\n",
    "- [Benchmarking Categorical Encoders](https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8)\n",
    "\n",
    "- [CategoricalEncodingBenchmark](https://github.com/DenisVorotyntsev/CategoricalEncodingBenchmark)\n",
    "\n",
    "Let's see how these methods work in this dataset.\n",
    "\n",
    "[Discussion](https://www.kaggle.com/c/cat-in-the-dat/discussion/112584)\n",
    "\n",
    "- no feature preprocessing\n",
    "- Use KFold(5) for CV (+ more fold get better score)\n",
    "- LR (C=0.1, solver=lbfgs)\n",
    "\n",
    "|Encoder|LB Score|\n",
    "|-|-|\n",
    "|TE|0.78018|\n",
    "|WOE|0.78861|\n",
    "|LOOE|0.79382|\n",
    "|James-Stein|0.77843|\n",
    "|Catboost|0.79164|\n",
    "|One-Hot(another my kernel)|0.77973|\n",
    "\n",
    "\n",
    "\n",
    "### Category-Encoders\n",
    "\n",
    "1. Label Encoder\n",
    "2. One-Hot Encoder\n",
    "3. Sum Encoder\n",
    "4. Helmert Encoder\n",
    "5. Frequency Encoder\n",
    "6. Target Encoder\n",
    "7. M-Estimate Encoder\n",
    "8. Weight Of Evidence Encoder\n",
    "9. James-Stein Encoder\n",
    "10. Leave-one-out Encoder\n",
    "11. Catboost Encoder\n",
    "---\n",
    "- Validation (Benchmark)\n",
    "    - single LR\n",
    "    - LR with Cross Validation\n",
    "\n",
    "- Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category-Encoders \n",
    "\n",
    "A set of scikit-learn-style transformers for encoding categorical variables into numeric by means of different techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# If you want to test this on your local notebook\n",
    "# http://contrib.scikit-learn.org/categorical-encoding/\n",
    "# !pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category-encoders\n",
      "  Downloading category_encoders-2.6.1-py2.py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from category-encoders) (1.3.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from category-encoders) (0.12.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from category-encoders) (1.3.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from category-encoders) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from category-encoders) (1.22.4)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from category-encoders) (0.5.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\samue\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category-encoders) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\samue\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category-encoders) (2.2.0)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders.james_stein import JamesSteinEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "\n",
    "TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read csv and doing some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "\n",
    "target = train['target']\n",
    "train_id = train['id']\n",
    "test_id = test['id']\n",
    "\n",
    "train.drop(['target', 'id'], axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(train.columns) # you can custumize later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notation\n",
    "\n",
    "- $y$ and $y+$ — the total number of observations and the total number of positive observations (y=1);\n",
    "- $x_i$, $y_i$ — the i-th value of category and target;\n",
    "- $n$ and $n+$ — the number of observations and the number of positive observations (y=1) for a given value of a categorical column;\n",
    "- $a$ — a regularization hyperparameter (selected by a user), prior — an average value of the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label Encoder (LE), Ordinary Encoder(OE)\n",
    "\n",
    "One of the most common encoding methods.\n",
    "\n",
    "An encoding method that converts categorical data into numbers.\n",
    "The code is very simple, and when you encode a specific column you can proceed as follows:\n",
    "\n",
    "``` python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label = LabelEncoder()\n",
    "\n",
    "train[column_name] = label.fit_transform(train[column_name])\n",
    "```\n",
    "\n",
    "The simple idea is to convert the same category to a number with the same value.\n",
    "\n",
    "So the range of numbers maps from 0 to n-1 as labels.\n",
    "\n",
    "The disadvantage is that the labels are ordered randomly (in the existing order of the data), which can add noise while assigning an unexpected order between labels. In other words, the data becomes ordinary (ordinal, ordered) data, which can lead to unintended consequences.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LE_encoder = OrdinalEncoder(feature_list)\n",
    "train_le = LE_encoder.fit_transform(train)\n",
    "test_le = LE_encoder.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-Hot Encoder (OHE, dummy encoder)\n",
    "\n",
    "\n",
    "So what can you do to give values ​​by category instead of ordering them?\n",
    "\n",
    "If you have data with specific category values, you can create a column. If the base Label Encoder label type is N, then OHE is the way to create N columns.\n",
    "\n",
    "Since only the row containing the content is given as 1, it is called one-hot encoding. Also called dummy encoding in the sense of creating a dummy.\n",
    "\n",
    "\n",
    "In this competition:\n",
    "\n",
    "``` python\n",
    "traintest = pd.concat([train, test])\n",
    "dummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\n",
    "train_ohe = dummies.iloc[:train.shape[0], :]\n",
    "test_ohe = dummies.iloc[train.shape[0]:, :]\n",
    "train_ohe = train_ohe.sparse.to_coo().tocsr()\n",
    "test_ohe = test_ohe.sparse.to_coo().tocsr()\n",
    "```\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.17 GiB for an array with shape (522, 300000) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\category_encoders\\utils.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;31m# for finding invariant columns transform without y (as is done on the test set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mX_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverride_return_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names_out_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_transformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\category_encoders\\utils.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, override_return_df)\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_invariants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverride_return_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\category_encoders\\one_hot.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Columns to be encoded can not contain new values'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\category_encoders\\one_hot.py\u001b[0m in \u001b[0;36mget_dummies\u001b[1;34m(self, X_in)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mbase_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[0mbase_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbase_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mold_column_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             new_data = concatenate_managers(\n\u001b[0m\u001b[0;32m    533\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.17 GiB for an array with shape (522, 300000) and data type int64"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#this method didn't work because of RAM memory. so we have to use pd.dummies \n",
    "OHE_encoder = OneHotEncoder(feature_list)\n",
    "train_ohe = OHE_encoder.fit_transform(train)\n",
    "test_ohe = OHE_encoder.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sum Encoder (Deviation Encoder, Effect Encoder)\n",
    "\n",
    "**Sum Encoder** compares the mean of the dependent variable (target) for a given level of a categorical column to the overall mean of the target. \n",
    "\n",
    "Sum Encoding is very similar to OHE and both of them are commonly used in Linear Regression (LR) types of models.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# this method didn't work because of RAM memory. \n",
    "# SE_encoder =SumEncoder(feature_list)\n",
    "# train_se = SE_encoder.fit_transform(train[feature_list], target)\n",
    "# test_se = SE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helmert Encoder\n",
    "\n",
    "**Helmert Encoding** is a third commonly used type of categorical encoding for regression along with OHE and Sum Encoding. \n",
    "\n",
    "It compares each level of a categorical variable to the mean of the subsequent levels. \n",
    "\n",
    "This type of encoding can be useful in certain situations where levels of the categorical variable are ordered. (not this dataset)\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# this method didn't work because of RAM memory. \n",
    "# HE_encoder = HelmertEncoder(feature_list)\n",
    "# train_he = HE_encoder.fit_transform(train[feature_list], target)\n",
    "# test_he = HE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Frequency Encoder\n",
    "\n",
    "This method encodes by frequency.\n",
    "\n",
    "Create a new feature with the number of categories from the training data.\n",
    "\n",
    "I will not proceed separately in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Encoder\n",
    "\n",
    "This is a work in progress for many kernels.\n",
    "\n",
    "The encoded category values are calculated according to the following formulas:\n",
    "\n",
    "$$s = \\frac{1}{1+exp(-\\frac{n-mdl}{a})}$$\n",
    "\n",
    "$$\\hat{x}^k = prior * (1-s) + s * \\frac{n^{+}}{n}$$\n",
    "\n",
    "- mdl means **'min data in leaf'**\n",
    "- a means **'smooth parameter, power of regularization'**\n",
    "\n",
    "Target Encoder is a powerful, but it has a huuuuuge disadvantage \n",
    "\n",
    "> **target leakage**: it uses information about the target. \n",
    "\n",
    "To reduce the effect of target leakage, \n",
    "\n",
    "- Increase regularization\n",
    "- Add random noise to the representation of the category in train dataset (some sort of augmentation)\n",
    "- Use Double Validation (using other validation)\n",
    "\n",
    "Let's use while being careful about overfitting.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "TE_encoder = TargetEncoder()\n",
    "train_te = TE_encoder.fit_transform(train[feature_list], target)\n",
    "test_te = TE_encoder.transform(test[feature_list])\n",
    "\n",
    "train_te.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. M-Estimate Encoder\n",
    "\n",
    "**M-Estimate Encoder** is a **simplified version of Target Encoder**. It has only one hyperparameter (Wrong Fomular but did good work?!)\n",
    "\n",
    "$$\\hat{x}^k = \\frac{n^+ + prior * m}{y^+ + m}$$\n",
    "\n",
    "The higher value of m results into stronger shrinking. Recommended values for m is in the range of 1 to 100.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MEE_encoder = MEstimateEncoder()\n",
    "train_mee = MEE_encoder.fit_transform(train[feature_list], target)\n",
    "test_mee = MEE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UPDATED : error founded in libarary https://github.com/scikit-learn-contrib/category_encoders/issues/200\n",
    "\n",
    "$$\\hat{x}^k = \\frac{n^+ + prior * m}{n+ + m}$$\n",
    "\n",
    "Thanks to [@ansh422](https://www.kaggle.com/ansh422)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Weight of Evidence Encoder \n",
    "\n",
    "**Weight Of Evidence** is a commonly used target-based encoder in credit scoring. \n",
    "\n",
    "It is a measure of the “strength” of a grouping for separating good and bad risk (default). \n",
    "\n",
    "It is calculated from the basic odds ratio:\n",
    "\n",
    "``` python\n",
    "a = Distribution of Good Credit Outcomes\n",
    "b = Distribution of Bad Credit Outcomes\n",
    "WoE = ln(a / b)\n",
    "```\n",
    "\n",
    "However, if we use formulas as is, it might lead to **target leakage**(and overfit).\n",
    "\n",
    "To avoid that, regularization parameter a is induced and WoE is calculated in the following way:\n",
    "\n",
    "$$nomiinator = \\frac{n^+ + a}{y^+ + 2*a}$$\n",
    "\n",
    "$$denominator = ln(\\frac{nominator}{denominator})$$\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "WOE_encoder = WOEEncoder()\n",
    "train_woe = WOE_encoder.fit_transform(train[feature_list], target)\n",
    "test_woe = WOE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. James-Stein Encoder\n",
    "\n",
    "**James-Stein Encoder** is a target-based encoder.\n",
    "\n",
    "The idea behind James-Stein Encoder is simple. Estimation of the mean target for category k could be calculated according to the following formula:\n",
    "\n",
    "$$\\hat{x}^k = (1-B) * \\frac{n^+}{n} + B * \\frac{y^+}{y} $$\n",
    "\n",
    "One way to select B is to tune it like a hyperparameter via cross-validation, but Charles Stein came up with another solution to the problem:\n",
    "\n",
    "$$B = \\frac{Var[y^k]}{Var[y^k] + Var[y]}$$\n",
    "\n",
    "Seems quite fair, but James-Stein Estimator has a big disadvantage — it is defined only for normal distribution (which is not the case for any classification task). \n",
    "\n",
    "To avoid that, we can either convert binary targets with a log-odds ratio as it was done in WoE Encoder (which is used by default because it is simple) or use beta distribution.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "JSE_encoder = JamesSteinEncoder()\n",
    "train_jse = JSE_encoder.fit_transform(train[feature_list], target)\n",
    "test_jse = JSE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Leave-one-out Encoder (LOO or LOOE)\n",
    "\n",
    "**Leave-one-out Encoding** is another example of target-based encoders.\n",
    "\n",
    "This encoder calculate mean target of category k for observation j if observation j is removed from the dataset:\n",
    "\n",
    "$$\\hat{x}^k_i = \\frac{\\sum_{j \\neq i}(y_j * (x_j == k) ) - y_i }{\\sum_{j \\neq i} x_j == k}$$\n",
    "\n",
    "While encoding the test dataset, a category is replaced with the mean target of the category k in the train dataset:\n",
    "\n",
    "$$\\hat{x}^k = \\frac{\\sum y_j * (x_j == k)  }{\\sum x_j == k}$$\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LOOE_encoder = LeaveOneOutEncoder()\n",
    "train_looe = LOOE_encoder.fit_transform(train[feature_list], target)\n",
    "test_looe = LOOE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Catboost Encoder\n",
    "\n",
    "**Catboost** is a recently created target-based categorical encoder. \n",
    "\n",
    "It is intended to overcome target leakage problems inherent in LOO. \n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CBE_encoder = CatBoostEncoder()\n",
    "train_cbe = CBE_encoder.fit_transform(train[feature_list], target)\n",
    "test_cbe = CBE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Validation proceeds with single lr and lr with cv.\n",
    "\n",
    "- I will add OneHotEncoder, etc later.\n",
    "- More Fold get better score (my experience)\n",
    "- you can try another solver and another parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "encoder_list = [ OrdinalEncoder(), WOEEncoder(), TargetEncoder(), MEstimateEncoder(), JamesSteinEncoder(), LeaveOneOutEncoder() ,CatBoostEncoder()]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=97)\n",
    "\n",
    "for encoder in encoder_list:\n",
    "    print(\"Test {} : \".format(str(encoder).split('(')[0]), end=\" \")\n",
    "    train_enc = encoder.fit_transform(X_train[feature_list], y_train)\n",
    "    #test_enc = encoder.transform(test[feature_list])\n",
    "    val_enc = encoder.transform(X_val[feature_list])\n",
    "    lr = LogisticRegression(C=0.1, solver=\"lbfgs\", max_iter=1000)\n",
    "    lr.fit(train_enc, y_train)\n",
    "    lr_pred = lr.predict_proba(val_enc)[:, 1]\n",
    "    score = auc(y_val, lr_pred)\n",
    "    print(\"score: \", score)\n",
    "    del train_enc\n",
    "    del val_enc\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR with CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# CV function original : @Peter Hurford : Why Not Logistic Regression? https://www.kaggle.com/peterhurford/why-not-logistic-regression\n",
    "\n",
    "def run_cv_model(train, test, target, model_fn, params={}, label='model'):\n",
    "    kf = KFold(n_splits=5)\n",
    "    fold_splits = kf.split(train, target)\n",
    "\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0]))\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print('Started {} fold {}/5'.format(label, i))\n",
    "        dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "        dev_y, val_y = target[dev_index], target[val_index]\n",
    "        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test, params)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        cv_score = auc(val_y, pred_val_y)\n",
    "        cv_scores.append(cv_score)\n",
    "        print(label + ' cv score {}: {}'.format(i, cv_score))\n",
    "        i += 1\n",
    "        \n",
    "    print('{} cv scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std score : {}'.format(label, np.std(cv_scores)))\n",
    "    pred_full_test = pred_full_test / 5.0\n",
    "    results = {'label': label, 'train': pred_train, 'test': pred_full_test, 'cv': cv_scores}\n",
    "    return results\n",
    "\n",
    "\n",
    "def runLR(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)[:, 1]\n",
    "    pred_test_y2 = model.predict_proba(test_X2)[:, 1]\n",
    "    return pred_test_y, pred_test_y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "\n",
    "    lr_params = {'solver': 'lbfgs', 'C': 0.1}\n",
    "\n",
    "    results = list()\n",
    "\n",
    "    for encoder in  [ OrdinalEncoder(), WOEEncoder(), TargetEncoder(), MEstimateEncoder(), JamesSteinEncoder(), LeaveOneOutEncoder() ,CatBoostEncoder()]:\n",
    "        train_enc = encoder.fit_transform(train[feature_list], target)\n",
    "        test_enc = encoder.transform(test[feature_list])\n",
    "        result = run_cv_model(train_enc, test_enc, target, runLR, lr_params, str(encoder).split('(')[0])\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results)\n",
    "    results['cv_mean'] = results['cv'].apply(lambda l : np.mean(l))\n",
    "    results['cv_std'] = results['cv'].apply(lambda l : np.std(l))\n",
    "    results[['label','cv_mean','cv_std']].head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even CVs did not solve the target based encoder's overfit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    for idx, label in enumerate(results['label']):\n",
    "        sub_df = pd.DataFrame({'id': test_id, 'target' : results.iloc[idx]['test']})\n",
    "        sub_df.to_csv(\"LR_{}.csv\".format(label), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
