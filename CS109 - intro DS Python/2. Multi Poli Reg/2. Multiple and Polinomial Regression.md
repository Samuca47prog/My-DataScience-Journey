# Multiple and Polinomial Regression

In An Introduction to Statistical Learning External link 2nd Edition, read the following sections:

3.2 Multiple Linear Regression (pages 71-72)
3.2.1 Estimating the Regression Coefficients (pages 72-75)
7.1 Polynomial Regression (pages 290-292)

## Notes
![Formula](images\formula.png)

## Some Important Questions
When we perform multiple linear regression, we usually are interested in
answering a few important questions.
1. Is at least one of the predictors X1, X2, . . . , Xp useful in predicting
the response?
2. Do all the predictors help to explain Y , or is only a subset of the
predictors useful?
3. How well does the model fit the data?
4. Given a set of predictor values, what response value should we predict,
and how accurate is our prediction?


# 2.1 Multilinear Regression

We will always want as many data as possible to build a model. 
If we get to the point that a feature seams to be insignificant, we just ignores it.

Vector notation is used for multilenar regression



# 2.2 Techniques for Multilinear Modeling

qualitative variables must be converted to dummy variables (one hot encoder)


## Type 1 and Type 2 Errors
Type of error matters

![Confusion matrix for desease model](images/confusion%20matrix%20for%20desease%20model.png)

More about the concept of Confusion Matrix in [Fundamentos de aprendizado de máquina: a matriz de confusão](https://www.youtube.com/watch?v=Kdsp6soqA7o)

How to plot a confusion matrix in Python? [How to Plot Confusion Matrix Heatmap in Python](https://www.youtube.com/watch?v=98LX2iRWXsY)


## Scaling: Normalization and Standardization

not needed for a true linear problem, but often required in a polynomial problem

Scelling: Min-Max Scaller, Standard Scaler
Normalization: Box Cox

Methods of both with code examples in [ChatGPT](https://chat.openai.com/share/7d5aa5d3-9921-4f1d-a487-29de3d57e3f4)

![Standarize and Normalize](images/standardize%20and%20normalize%20.png)

Explanation on: [Standardization vs Normalization Clearly Explained!](https://www.youtube.com/watch?v=sxEqtjLC0aM)

* in scaling, you're changing the range of your data, while
* in normalization, you're changing the shape of the distribution of your data.

<br>
<b>Below I will share some basic tips that you can use while trying to scale:</b><br>
    1. If you do not know which scaler to use, apply all and check the effect on the models. <br> 
    2. If you do not understand the data, use standard scaler. It works most of the times. <br> 
    3. If you know the max and min values of the feature, then use min max scaler. Like in CNN. <br> 
    4. If most of the values in the feature column is 0 or sparce matrix, then use Max Absolute Scaling<br> 
    5. If the data has outliers, use Robust Scaling.<br> <br> <br> 

## Multicolinearity
Multicolinearity makes hard the job to determine what input had more influence in outcome

Generaly, we would like to keep just one of the colinear variables

use pairplots to check that 

more on Multicolinearity: [Multicollinearity (in Regression Analysis)](https://www.youtube.com/watch?v=G1WX5GiFSWQ)

VIF is a measure of multicolinearity. More than 10 indicates that.

doesn't affect prediction, but must be handled if determinination of what features is important is needed

# 2.3 Polynomial Regression

It is often a good idea to scale the input of a polynomial regression. To large or too small numbers to the 20th power will underflow or overflows the math.

how to implment Polynomial Regression using SkLearn: [Machine Learning with Scikit-Learn Python | Polynomial Linear Regression](https://www.youtube.com/watch?v=wi6VoJcLyag)


## Underfitting and Overfitting




# Practices

## Features: Must We Pick Just One?

## Fitting a Multi-Regression Model

## Creating Dummy/Indicator Variables

## Features on Different Scales

## Multi-collinearity vs Model Predictions

## A Line Won't Cut It

## Polynomial Modeling
sometimes we can get one more coeficients than expected. This can happen because sklearn transformations include a column with 1s for the y intercept.
Solve that with: include_bias=False




# Kaggle
Projects that I looked over to see these topics in practice

Multilenear projects
* [Fish Market-Multilinear Regression](https://www.kaggle.com/code/hakansaritas/fish-market-multilinear-regression)
  * > Skewed analisys, Multicolinearity. Models with SMF, Sckitlearn. Visualization of predicted Weight

* [The power of normalization and visualization](https://www.kaggle.com/code/fightingmuscle/the-power-of-normalization-and-visualization?scriptVersionId=92329738)
  * > A lot of graph views, few explanation but many visualizations.

* [Bike Sharing : Multiple Linear Regression](https://www.kaggle.com/code/gauravduttakiit/bike-sharing-multiple-linear-regression)
  * > A good example for business report. Build a business scenario, has explanations. Analysis of coeficients in the end and final Report.

Scaling and normalization
* [Scaling and Normalization](https://www.kaggle.com/code/alexisbcook/scaling-and-normalization)
  * > Clear explanation and simple implementation of Scaling and Normalization

* [Complete guide to Feature Scaling](https://www.kaggle.com/code/aimack/complete-guide-to-feature-scaling?scriptVersionId=70328903)
  * > More practical code and visuable kernel about feature Scaling.


Encoders
* [11 Categorical Encoders and Benchmark](https://www.kaggle.com/code/subinium/11-categorical-encoders-and-benchmark)
  * > Shows 11 encoders types, a little explanation on each one, compare results.

Confusion Matrix
* [Bank Marketing + Classification + ROC,F1,RECALL...](https://www.kaggle.com/code/henriqueyamahata/bank-marketing-classification-roc-f1-recall)
  * > 

Multicolinearity
* [Multicolinearity Problem | SVM (RBF)](https://www.kaggle.com/code/heitornunes/multicolinearity-problem-svm-rbf)
  * > 

Polinomial
* [Polynomial Regression](https://www.kaggle.com/code/aminizahra/polynomial-regression)
  * > 
* [Polynomial Regression| Regularization| Assumptions](https://www.kaggle.com/code/farzadnekouei/polynomial-regression-regularization-assumptions)
  * > 


Extra:
* [Forecasting of Bitcoin Prices](https://www.kaggle.com/code/ara0303/forecasting-of-bitcoin-prices)
  * > 

* [Titanic Data Science Solutions](https://www.kaggle.com/code/startupsci/titanic-data-science-solutions)
  * > 

## My Project
 